# -*- coding: utf-8 -*-
"""NLP - DataPreprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IsehyRWKpn_DCEG0VvViL_UMaZDOJIWJ

#Making the texts informative
"""

# data containing english texts
df = pd.read_csv("mydata")

import nltk
nltk.download('stopwords')

from num2words import num2words 
from nltk.stem import WordNetLemmatizer 
from nltk.stem import PorterStemmer 
from nltk.corpus import stopwords
import operator
from spellchecker import SpellChecker

#Remove URLs
def remove_urls(data):
    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    return url_pattern.sub(r'', data)

#Correcting spellings
def correct_spellings(data):
    spell = SpellChecker()
    corrected_text = []
    misspelled_words = spell.unknown(data.split())
    for word in data.split():
        if word in misspelled_words:
            corrected_text.append(spell.correction(word))
        else:
            corrected_text.append(word)
    return " ".join(corrected_text)

def convert_lower_case(data):
  return str(data).lower()

#Removing the punctuations
def remove_punctuation(data):
  symbols = string.punctuation
  for symbol in symbols:
    data = str(data).replace(symbol, '')
  return data

#Remove single words a they are meaningless
def remove_single_characters(data):
  new_data = ""
  for word in data.split():
    if len(word) > 1:
      new_data = new_data + " " + word
  return new_data

#Convert numbers into words
def convert_numbers(data):
  new_data = ""
  for word in data.split():
    if(word.isnumeric()):
      word = num2words(word)
    new_data = new_data + " " + word
  return new_data

#Stemming the dataset
def stemming(data):
  stemmer = PorterStemmer()
  new_data = ""
  for word in data.split():
    word = stemmer.stem(word)
    new_data = new_data + " " + word
  return new_data

#Lemmatizing the dataset
def lemmatizer(data):
  lemmatizer = WordNetLemmatizer()
  new_data = ""
  for word in data.split():
    word = lemmatizer.lemmatize(word)
    new_data = new_data + " " + word
  return new_data

#Preprocessing-01
def preprocessing1(dataset):
  new_dataset = []
  for data in dataset:
    #data = remove_urls(data)
    #data = correct_spellings(data)
    data = convert_lower_case(data)
    data = remove_punctuation(data)
    data = remove_single_characters(data)
    data = convert_numbers(data)
    data = remove_punctuation(data)
    data = convert_numbers(data)
    new_dataset.append(data)
  return new_dataset

df['Text'] = preprocessing1(df['Text'])
##################################################
#Preprocessing for removing stopwords

#Building a list of stopwords from the data using Zipf' Law
def buildstoplist(trainingdata):
    index = {}
    wordcount = 0
    wordprob = {}
    stop_words = list(stopwords.words('english')) #Calling predefined stopwords list from nltk
    for i in trainingdata:
        for word in i[0].split():
            word = re.sub('[^a-zA-Z0-9 \n\.]', '', word)
            if word != '':
                wordcount += 1
                if word not in index:
                    index[word] = 1
                else:
                    index[word] += 1
    
    for word in index:
        wordprob[word] = round(float(index[word])/wordcount*100,4) #Calculating the probability
        
    stoplist = sorted(wordprob, key=operator.itemgetter(1) ,reverse=True)[:25000] #Storing top 25000 most frequent words of the data
    
    for i in stoplist:
        stop_words.append(i[0]) #Making a list od stopwords
    stop_words.append('reuters')
    return stop_words
  
stopwords = buildstoplist(df['Text'])

#Removing stopwords from the dataset
def remove_stopwords(data):
  new_data = ""
  for word in data.split():
    if word not in stopwords:
      new_data = new_data + " " + word
  return new_data

#Preprocessing-02
def preprocessing2(dataset):
  new_dataset = []
  for data in dataset:
    #data = remove_urls(data)
    #data = correct_spellings(data)
    data = remove_stopwords(data)
    new_dataset.append(data)
  return new_dataset

df['Text'] = preprocessing2(df['Text'])
df['Text'].head()